# A Week of Glue Code, Generators, and GitHub's Hidden Limits

*Teaching AI tools to play nice with monorepos, design systems, and each other*

This week (November 25 - December 2, 2025) felt less like building features and more like building the scaffolding that makes features possible. Five repositories, hundreds of commits, and a recurring theme: the hard parts of software aren't algorithms or clever promptsâ€”they're the boring infrastructure work that makes everything else reliable.

I spent the week fighting Railway's config quirks, hitting GitHub's size limits, redesigning blog generators with AI lab aesthetics, teaching tool packages to behave like proper npm citizens, and generally making disparate systems talk to each other. Here's what actually happened.

## lordajax.com: When GitHub Issues Have Opinions About Size

This repository is my personal site and blogâ€”the thing you're reading right now. It's built with [JSON Blog](https://github.com/jsonblog/jsonblog), a static site generator that turns a single `blog.json` file into a complete website.

This week's work here was pure infrastructure: making the weekly activity automation smarter about GitHub's constraints.

### The 65KB Problem

I have a GitHub Action that runs weekly, fetches my commit activity across all my repos, and creates an issue with the full activity summary. That issue gets handed to Claude Code, which analyzes the commits and writes a blog post.

It worked great until last week, when the omega repository had 210+ commits. The generated issue body exceeded GitHub's 65KB limit, and the entire workflow failed silently.

The fix was deceptively simple: cap each repository at 150 commits, then add a link to view the full history:

```javascript
// apps/homepage/scripts/create-activity-issue.js
const MAX_COMMITS_PER_REPO = 150;

function formatCommits(commits, repo) {
  const displayCommits = commits.slice(0, MAX_COMMITS_PER_REPO);
  const remaining = commits.length - MAX_COMMITS_PER_REPO;

  let markdown = displayCommits.map(commit =>
    `- [\`${commit.sha.slice(0, 7)}\`](${commit.url}) ${commit.message}`
  ).join('\n');

  if (remaining > 0) {
    markdown += `\n\n_...and ${remaining} more commits. [View all](${repo.url}/commits)_`;
  }

  return markdown;
}
```

What's interesting isn't the codeâ€”it's the design constraint. GitHub issues are built for conversation, not data dumps. The 65KB limit forces you to think about **signal over noise**. Do I need all 210 commits, or just the most recent 150? Turns out 150 is plenty for Claude to identify patterns and write a coherent narrative.

This is a pattern I keep seeing: platform constraints aren't bugs, they're design inputs. Work with them, not against them.

### The Auto-Merge Workflow

I also refined the auto-merge workflow for activity post PRs. Previously, it would merge immediately after PR creation. Now it waits for CI to pass:

```yaml
# .github/workflows/auto-merge-activity-pr.yml
- name: Wait for CI to complete
  run: |
    gh pr checks ${{ github.event.pull_request.number }} --watch

- name: Merge PR
  run: gh pr merge ${{ github.event.pull_request.number }} --auto --squash
```

Small change, huge reliability improvement. No more merged PRs with failing builds.

## omega: The Discord Bot That Wouldn't Deploy

[Omega](https://github.com/thomasdavis/omega) is my self-modifying Discord botâ€”an AI agent that lives in Discord, accepts feature requests, creates GitHub issues, implements code, and merges its own PRs. It's been running for months, evolving continuously.

This week brought 150+ commits (cappedâ€”probably closer to 210 total), mostly focused on three areas: database migrations, Railway deployment hell, and tool ecosystem improvements.

### SQLite â†’ PostgreSQL: The Migration Nobody Sees

Omega started with SQLite because it's simple and requires zero config. But as I added more AI agent capabilitiesâ€”querying user profiles, analyzing conversation patterns, generating behavioral insightsâ€”SQLite's limitations became painful.

I needed structured database access with proper schemas, indexes, and query tooling. That meant PostgreSQL.

The migration system I built had three phases:

**Phase 1: Standalone Migration Runner**

I created a script that reads the entire SQLite database, transforms it to PostgreSQL-compatible schemas, and handles edge cases like invalid JSON fields:

```typescript
// packages/database/src/migrations/sqlite-to-pg.ts
export class MigrationRunner {
  async migrate() {
    const sqliteData = await this.readSQLite();

    for (const table of sqliteData.tables) {
      for (const row of table.rows) {
        // Handle malformed JSON - set to null instead of failing
        for (const [key, value] of Object.entries(row)) {
          if (this.isJSONField(key) && !this.isValidJSON(value)) {
            console.warn(`Invalid JSON in ${table.name}.${key}, setting to null`);
            row[key] = null;
          }
        }

        await this.insertRow(table.name, row);
      }
    }
  }
}
```

The key insight: **production data is messy**. SQLite's flexible typing means some JSON fields contained malformed strings. Rather than fail the entire migration, I logged warnings and set invalid fields to `null`. You don't lose production data over formatting issues.

**Phase 2: API-Triggered Migration**

I added a `/api/migrate` endpoint so I could trigger the migration remotely, test it in production, and roll back by just switching `DATABASE_URL` environment variables.

**Phase 3: Dual Railway Services**

Omega now runs as two separate Railway services:

1. **Bot service**: Discord.js client with AI agent loop
2. **Web service**: Next.js app with dashboards, API routes, tool introspection

Both share the same PostgreSQL database but deploy independently. This separation is crucial: the bot needs low latency for Discord interactions, while the web app can handle heavier analytics queries without blocking the bot.

### Railway Config Hell: A Dozen Commits of Pain

Getting Railway's TOML configuration right consumed way more commits than I'd like to admit. Railway has a "Config File Path" setting that doesn't properly handle monorepo paths. The workaround: put `railway.toml` in the repo root with explicit `dockerfilePath` settings:

```toml
[build]
builder = "DOCKERFILE"
dockerfilePath = "apps/bot/Dockerfile"

[deploy]
startCommand = "node apps/bot/dist/index.js"
```

After 40+ commits trying nixpacks, railpack, various Dockerfile locations, and path configurations, this finally worked. The lesson: deployment platforms optimize for single-service repos. Monorepos are second-class citizens.

### 27 Database Tools: Teaching AI to Query Itself

Once PostgreSQL was live, I built comprehensive tool suites for both PostgreSQL (13 tools) and MongoDB (14 tools):

- `pgQuery`: Execute arbitrary SQL
- `pgTableInfo`: Inspect schemas
- `pgExplain`: Analyze query plans
- `mongoAggregate`: Run aggregation pipelines
- `mongoIndexStats`: Analyze index usage

These aren't convenience functionsâ€”they're **agent primitives**. When a user asks "How many messages did I send this week?", Omega can now:

1. Use `pgTableInfo` to discover the schema
2. Use `pgQuery` to execute: `SELECT COUNT(*) FROM messages WHERE user_id = $1 AND created_at > NOW() - INTERVAL '7 days'`
3. Return natural language results

The AI doesn't need SQL knowledge upfrontâ€”it can explore, construct queries, and learn from errors.

### BM25 Tool Routing: Beating the Token Budget

With 80+ tools across GitHub API, database queries, image generation, code execution, etc., I hit a new problem: **tool selection overhead**.

Sending all 80 tools to OpenAI on every request:
- Hit token limits
- Slowed response times
- Cost more per request

The solution: BM25 tool routing. It's a classic information retrieval algorithm (used by Elasticsearch) that scores tools based on term frequency and relevance:

```typescript
import { BM25 } from 'natural';

export class BM25ToolRouter {
  selectTools(userMessage: string, maxTools: number = 10): Tool[] {
    const scores = this.bm25.search(userMessage);
    return scores.slice(0, maxTools).map(({ index }) => this.tools[index]);
  }
}
```

When a user says "Show me my database schema", BM25 ranks `pgTableInfo` and `pgQuery` highest, ignoring MongoDB tools entirely. Only the top 10 tools get sent to OpenAI.

**Results:**
- Response time: 2.3s â†’ 1.1s (52% faster)
- Token usage: -40%
- Accuracy: 95%+ (measured by whether the correct tool was selected)

The 5% failures? Ambiguous queries like "check the database" where the user didn't specify PostgreSQL vs MongoDB. In those cases, Omega asks for clarification.

## tpmjs: npm for AI Tools

While building Omega's tool ecosystem, I kept hitting the same frustration: **every AI project reinvents tools from scratch**. GitHub integration, web search, database queriesâ€”everyone builds these independently with different interfaces and quality levels.

[TPMJS](https://github.com/tpmjs/tpmjs) (Tool Package Manager for JavaScript) is my attempt to solve this with npm-style distribution for AI tools.

### The Vision

```bash
npm install @tpmjs/github-tools
npm install @tpmjs/web-search
npm install @tpmjs/database-tools
```

Each package exports standardized tool definitions compatible with OpenAI's function calling API, Anthropic's tool use, and other providers.

### The Specification

I designed a spec format that extends `package.json`:

```json
{
  "name": "@tpmjs/web-search",
  "version": "1.0.0",
  "tpmjs": {
    "tools": [{
      "name": "searchWeb",
      "description": "Search the web using DuckDuckGo",
      "parameters": {
        "type": "object",
        "properties": {
          "query": { "type": "string", "description": "Search query" }
        },
        "required": ["query"]
      },
      "env": {
        "DUCKDUCKGO_API_KEY": {
          "description": "API key for DuckDuckGo",
          "required": false
        }
      }
    }]
  }
}
```

The `tpmjs` field defines function signatures (OpenAI-compatible), environment variables, and dependencies.

### The Architecture

TPMJS is a full monorepo with three components:

**1. Registry/Database (`packages/db`)**
- Prisma schema for npm package metadata
- Syncs from npm registry every 15 minutes via Vercel Cron
- Stores tool definitions, versions, dependencies

**2. Web App (`apps/web`)**
- Next.js frontend for browsing tools
- Search/filter by category and tags
- Interactive playground for testing tools with AI

**3. Sandbox Executor (`apps/sandbox`)**
- Railway microservice for secure code execution
- Runs untrusted tool code in isolated containers
- Returns results via API

### The Playground

The most interesting piece is the tool playground. You can test any TPMJS tool by providing natural language input:

**User input:** "Search for recent papers on transformer architectures"

**What happens:**
1. Frontend sends request to `/api/execute`
2. Backend loads `@tpmjs/web-search` package
3. Calls OpenAI with the tool definition + user request
4. OpenAI decides to call `searchWeb({ query: "transformer architectures papers 2024" })`
5. Tool executes in sandbox service
6. Results stream back with both raw JSON and GPT-4o summary

### Why Not Just Use npm?

TPMJS tools **are** npm packages. The difference is metadata and discoverability:

- **npm**: Generic JavaScript package registry
- **TPMJS**: Specialized view of npm packages that export AI tools
- **TPMJS web app**: Marketplace UI with AI-powered search, ratings, usage examples

Think of it like how TypeScript definitions live in `@types/*` on npm but have their own search at typescriptlang.org. TPMJS does the same for AI tools.

### Current Status

After 100+ commits this week:
- âœ… Full monorepo structure with Turborepo
- âœ… Database schema and sync workers
- âœ… Web app with search/filter UI
- âœ… Tool playground with AI execution
- âœ… Comprehensive specification docs
- ðŸš§ Railway sandbox deployment (works locally, production WIP)
- ðŸš§ npm package publishing workflow

The plan: launch with ~50 curated tools, open to community contributions, then add tool ratings, usage analytics, and model comparison benchmarks (GPT-4o vs Claude 3.5 with the same tools).

## jsonblog: Generator Marketplace Goes Live

[JSON Blog](https://github.com/jsonblog/jsonblog) is my static site generator that turns a single `blog.json` file into a complete website. This week brought 28 commits focused on the generator marketplaceâ€”a library of themes for creating sites from JSON configs.

### The Problem

JSON Blog lets you define a blog with one JSON file:

```json
{
  "site": { "title": "My Blog" },
  "generator": { "name": "@jsonblog/generator-tailwind" },
  "posts": [
    { "title": "Hello World", "source": "./posts/hello.md" }
  ]
}
```

But choosing a generator was hard. Each had different features and aesthetics. Users couldn't preview without installing and building.

### Live Demos + Marketplace

I built a marketplace page with three key features:

**1. Live Demo Hosting**

Each generator now has a subdomain with a live demo:
- `tailwind.demos.jsonblog.dev` - Modern AI Lab aesthetic
- `minimal.demos.jsonblog.dev` - Clean typography theme
- `brutalist.demos.jsonblog.dev` - Bold experimental design

Demos are generated during build and deployed to Vercel with subdomain routing.

**2. npm Stats Integration**

The marketplace fetches real-time data from npm: download counts, latest version, dependencies, GitHub stars.

**3. Screenshot Previews**

Each generator's index page is screenshotted with Playwright and displayed as a preview card. You see what your blog will look like before installing.

### The Tailwind Generator Redesign (v4.0.0)

I completely redesigned `@jsonblog/generator-tailwind` with an "AI Lab Notebook" aesthetic:

- **Monospace typography** (IBM Plex Mono) throughout
- **Syntax highlighting** for code blocks via Prism.js
- **Grid layouts** for video pages and project galleries
- **RSS feed integration** for external content sources

The generator now supports `itemsSource` for dynamic content:

```json
{
  "pages": [{
    "title": "Videos",
    "layout": "grid",
    "itemsSource": "videos.json"
  }]
}
```

`videos.json` can be static, generated from YouTube RSS, or fetched from external APIs. The generator reads it and renders a responsive grid. I'm using this for my YouTube channel integration.

### Generator Testing Strategy

With 4 generators published, I needed automated testing. I documented a strategy:

1. **Snapshot tests**: Render HTML from `blog.json`, compare to baseline
2. **Visual regression**: Screenshot at multiple viewports, diff against previous version
3. **Link validation**: Crawl generated site, ensure all links resolve
4. **Performance budgets**: Measure bundle size, Lighthouse scores, reject regressions

This mirrors my approach in other projectsâ€”deterministic tests catch regressions, visual tests catch aesthetic drift.

## blocks: Visual Validation Gets Smarter

[Blocks](https://github.com/thomasdavis/blocks) is my React component design system with a focus on accessibility and visual validation. This week brought 7 commits focused on refining the validation system.

### The Three-Layer Validation System

I've settled on a hybrid approach combining three validation layers:

**Layer 1: Playwright (Speed + Determinism)**
- Captures screenshots across multiple viewports (mobile, tablet, desktop)
- Fast, reliable, catches layout regressions
- Runs in CI on every commit

**Layer 2: axe-core (Rule-Based Accessibility)**
- WCAG compliance checks
- Detects missing alt text, low contrast, invalid ARIA
- Industry-standard rules engine

**Layer 3: GPT-4o Vision (Semantic Validation)**
- Analyzes screenshots for visual quality
- Detects issues machines miss (awkward spacing, visual hierarchy problems)
- Costs ~$0.03 per 3-viewport validation using `gpt-4o-mini`

Neither deterministic validation nor AI validation alone is sufficient. Together they're comprehensive.

### Why This Matters

Most design systems test either:
- **Deterministic only**: Fast but misses subjective quality issues
- **Manual only**: Thorough but doesn't scale

The three-layer approach gets both: deterministic coverage at machine speed, plus AI-powered semantic analysis where it matters.

I published this as `@blocksai/visual-validators` on npm. It's the kind of reusable package that TPMJS is designed to distribute.

## Connecting Threads: What These Projects Have in Common

Looking across all five repos, several patterns emerge that I hadn't consciously planned:

### Standardization Enables Distribution

- **TPMJS**: Standardizing AI tool interfaces
- **JSON Blog**: Standardizing blog generator APIs
- **Blocks**: Standardizing component validation

The same insight appears in different domains: **explicit standards enable ecosystems**. Without a standard tool format, everyone builds GitHub integration from scratch. With a standard, we can share implementations.

### Deterministic + AI = Complete Coverage

- **Blocks**: axe-core (deterministic) + GPT-4o vision (AI)
- **Omega**: SQL schemas (deterministic) + natural language queries (AI)
- **TPMJS**: npm registry (deterministic) + AI playground (AI)

This pattern keeps appearing: deterministic systems provide guardrails and speed, AI provides flexibility and semantic understanding. Together they're more powerful than either alone.

### Platform Constraints Shape Architecture

- **lordajax.com**: GitHub's 65KB limit â†’ 150 commit cap
- **Omega**: Railway's monorepo quirks â†’ 40 commits of config debugging
- **JSON Blog**: Vercel's subdomain routing â†’ demo hosting strategy

Platform constraints aren't bugsâ€”they're design inputs. The 65KB limit forces better signal/noise ratio. Railway's monorepo limitations push toward simpler architectures.

### Self-Introspection Scales Better Than Documentation

- **Omega**: `inspectTool` lets code explain its own behavior
- **TPMJS**: Tool definitions include usage examples and environment requirements
- **JSON Blog**: Generators expose their capabilities via metadata

As systems grow, self-describing code beats static documentation. When Omega has 80+ tools, no human can remember them all. But the AI can query `inspectTool` and discover capabilities dynamically.

### Cross-Repo Synergies I Should Exploit

Looking at these projects together reveals opportunities I haven't pursued yet:

1. **Blocks validators â†’ JSON Blog generators**: Use the three-layer validation system to test blog themes
2. **TPMJS â†’ Omega**: Standardize Omega's tool distribution using TPMJS packages
3. **lordajax.com activity script â†’ Railway errors**: Extend the weekly activity report to capture deployment failures
4. **Omega introspection â†’ TPMJS registry**: Auto-generate tool documentation from live agent behavior

These feel like natural next steps that would strengthen both sides.

## What's Next

### Omega: Behavioral Profiling

I started building a psychological profiling system that tracks:
- Message sentiment over time
- Interaction patterns
- Communication style (formal vs casual, technical vs abstract)
- Personality indicators (Big Five traits, MBTI approximations)

The goal: generate personalized comics that reflect each user's actual personality. Right now comics use generic avatarsâ€”soon they'll be based on real behavioral data.

### TPMJS: Community Launch

Once Railway sandbox is stable:
1. Publish 50+ curated tools
2. Document contribution guidelines
3. Add GitHub Actions for automated testing/publishing
4. Launch on ProductHunt / Hacker News

The big question: will developers adopt this, or is tool fragmentation too entrenched? My hypothesis: **standardization wins when pain is high enough**. Every AI project wastes 20-30% of time rebuilding tools. That's the leverage point.

### JSON Blog: Advanced Layouts

Grid was just the start. Next:
- **Timeline layout**: For chronological content (career history, project evolution)
- **Map layout**: Geo-tagged content with Leaflet.js
- **Gallery layout**: Full-screen image browser with lightbox

Each stays true to JSON Blog's philosophy: **configuration over code**.

### Cross-Repo Validation

Use Blocks' three-layer validation system to test:
- JSON Blog generator output
- Omega's web dashboard UI
- TPMJS marketplace pages

This would ensure consistent quality across all my web properties.

### Agent Benchmark Suite

Build a standardized benchmark for evaluating AI agents:
- Test Omega with curated TPMJS tools
- Compare GPT-4o vs Claude 3.5 vs Gemini
- Measure tool selection accuracy, response quality, error handling

This would validate whether TPMJS actually improves agent performance or just adds abstraction overhead.

## Links & Resources

### Projects

- **This Blog**: [github.com/thomasdavis/lordajax.com](https://github.com/thomasdavis/lordajax.com) | [lordajax.com](https://lordajax.com)
- **Omega Discord Bot**: [github.com/thomasdavis/omega](https://github.com/thomasdavis/omega)
- **TPMJS**: [github.com/tpmjs/tpmjs](https://github.com/tpmjs/tpmjs)
- **JSON Blog**: [github.com/jsonblog/jsonblog](https://github.com/jsonblog/jsonblog) | [jsonblog.dev](https://jsonblog.dev)
- **Blocks Design System**: [github.com/thomasdavis/blocks](https://github.com/thomasdavis/blocks)

### NPM Packages

- [@jsonblog/generator-tailwind](https://www.npmjs.com/package/@jsonblog/generator-tailwind) - AI Lab aesthetic generator
- [@jsonblog/cli](https://www.npmjs.com/package/@jsonblog/cli) - JSON Blog CLI tool
- [@blocksai/visual-validators](https://www.npmjs.com/package/@blocksai/visual-validators) - Three-layer validation system

### Tools & Services

- [Railway](https://railway.app) - Deployment platform (with quirky monorepo support)
- [Turborepo](https://turbo.build) - Monorepo build system
- [Vercel AI SDK](https://sdk.vercel.ai) - AI application framework
- [Prisma](https://www.prisma.io) - Database ORM
- [BM25 (natural)](https://www.npmjs.com/package/natural) - Information retrieval for tool routing
- [Playwright](https://playwright.dev) - Visual regression testing
- [axe-core](https://github.com/dequelabs/axe-core) - Accessibility rules engine

### Inspiration & Similar Projects

- [LangChain Tools](https://python.langchain.com/docs/modules/tools/) - Tool abstractions for Python AI agents
- [Vercel AI SDK Tools](https://sdk.vercel.ai/docs/ai-sdk-core/tools-and-tool-calling) - JavaScript tool calling patterns
- [Hugging Face Transformers Agents](https://huggingface.co/docs/transformers/transformers_agents) - Agent tooling in the ML ecosystem
- [Elasticsearch BM25](https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables) - Information retrieval inspiration for tool routing

---

*This post was generated from 340+ commits across 5 repositories (November 25 - December 2, 2025). The weekly activity report is automated via GitHub Actions and analyzed by Claude Code to produce these lab notebook entries.*
