# Weekly Activity: Building Collaborative AI Systems with Real-Time Document Editing

This week focused on pushing the boundaries of human-AI collaboration through real-time collaborative features, exploring AI emotion systems, and refining development frameworks. The main work spanned three major projects: adding Google Docs-like collaboration to Omega (the self-coding Discord bot), enhancing Blocks with visual validation capabilities, and infrastructure improvements across multiple platforms.

## Omega: From Solo AI to Collaborative Intelligence

[Omega](https://github.com/thomasdavis/omega) is a Discord bot that writes its own code. This week, the most significant evolution was adding **real-time collaborative document editing**‚Äîessentially building "Google Docs for AI agents."

### The Vision: Shared Workspace for Humans and AI

The goal was to enable multiple users and AI agents to collaborate on documents in real-time. Think of it as a shared whiteboard where humans can brainstorm and AI can contribute, with everyone seeing changes instantly.

This required integrating:
- **Yjs** - Conflict-free Replicated Data Type (CRDT) library for real-time collaboration
- **Pusher** - WebSocket infrastructure for broadcasting updates
- **Express server** - HTTP API for document management

### Implementing Yjs for Conflict-Free Collaboration

Yjs is a CRDT library that allows multiple clients to edit the same document simultaneously without conflicts. The key insight of CRDTs is that operations are commutative‚Äîthe order doesn't matter, the final state is always consistent.

Here's how Omega applies Yjs updates from collaborators:

```typescript
export async function applyYjsUpdate(
  documentId: string,
  update: Uint8Array,
  clientId: string
): Promise<void> {
  console.log('üîÑ applyYjsUpdate called');
  console.log('   Document ID:', documentId);
  console.log('   Client ID:', clientId);
  console.log('   Update size:', update.length);

  const ydoc = await getOrCreateYDoc(documentId);

  // Apply the update to our local Yjs document
  Y.applyUpdate(ydoc, update);

  // Persist the updated state
  await saveYDocState(documentId, Y.encodeStateAsUpdate(ydoc));
}
```

The beauty of this approach is that each client maintains its own Yjs document, applies updates as they arrive, and the CRDT algorithm ensures consistency across all clients.

### Real-Time Presence with Pusher

To broadcast changes to all connected clients, Omega uses Pusher channels. When a user edits a document, the update is broadcast to everyone viewing that document:

```typescript
export async function broadcastPresence(
  documentId: string,
  data: {
    action: 'join' | 'leave' | 'update';
    userId: string;
    username?: string;
    timestamp: number;
  }
): Promise<void> {
  console.log('üì° broadcastPresence called');
  console.log('   Document ID:', documentId);
  console.log('   Action:', data.action);
  console.log('   User ID:', data.userId);

  const channelName = `presence-document-${documentId}`;

  await pusher.trigger(channelName, 'presence-change', {
    action: data.action,
    userId: data.userId,
    username: data.username,
    timestamp: data.timestamp,
  });
}
```

The Pusher configuration is pulled from environment variables:

```typescript
export function getPusherConfig(): {
  key: string;
  cluster: string;
} {
  const key = process.env.PUSHER_KEY;
  const cluster = process.env.PUSHER_CLUSTER || 'us2';

  console.log('üîç DEBUG: getPusherConfig called');
  console.log('   PUSHER_KEY:', key ? `${key.substring(0, 5)}...` : 'NOT SET');
  console.log('   PUSHER_CLUSTER:', cluster);

  return { key, cluster };
}
```

### Document Server API

The artifact server exposes several endpoints for document management:

**Create/Get Document:**
```typescript
app.get('/api/documents/:id', async (req: Request, res: Response) => {
  const { id } = req.params;
  const document = await getDocument(id);
  res.json(document);
});
```

**User Presence:**
```typescript
app.post('/api/documents/:id/join', async (req: Request, res: Response) => {
  const { id } = req.params;
  const { userId, username } = req.body;

  console.log('üë§ User joining document');
  console.log('   Document ID:', id);
  console.log('   User ID:', userId);
  console.log('   Username:', username);

  await broadcastPresence(id, {
    action: 'join',
    userId,
    username,
    timestamp: Date.now(),
  });

  res.json({ success: true });
});
```

**Yjs Updates:**
```typescript
app.post('/api/documents/:id/yjs', async (req: Request, res: Response) => {
  const { id } = req.params;
  const { update, clientId } = req.body;

  console.log('üì° Yjs update received');
  console.log('   Document ID:', id);
  console.log('   Client ID:', clientId);

  const updateArray = new Uint8Array(Object.values(update));
  await applyYjsUpdate(id, updateArray, clientId);

  res.json({ success: true });
});
```

**Plain Text Export:**
```typescript
app.get('/api/documents/:id/plain', async (req: Request, res: Response) => {
  const { id } = req.params;
  const document = await getDocument(id);

  // Return raw text content for copying/downloading
  res.type('text/plain');
  res.send(document.content);
});
```

### Why This Matters

This collaborative document feature transforms Omega from a single-agent system into a **multi-agent collaborative workspace**. Now you can:

1. Start a brainstorming session in Discord
2. Have Omega create a shared document
3. Invite team members to collaborate in real-time
4. Let AI agents contribute alongside humans
5. Export the final document as plain text or formatted content

This is particularly powerful for:
- **Product specifications** - Humans outline requirements, AI fills in technical details
- **Code reviews** - AI explains code, humans ask questions in-document
- **Research synthesis** - Multiple AI agents contribute different perspectives
- **Technical documentation** - Collaborative writing with AI suggestions

### Related Issues Opened

Several issues were opened this week to extend the collaborative features:

- [**Issue #331**](https://github.com/thomasdavis/omega/issues/331): Add a tool to automatically create live documents from user requests
- [**Issue #329**](https://github.com/thomasdavis/omega/issues/329): Enable prepopulated live document creation for collaborative Q&A
- [**Issue #307**](https://github.com/thomasdavis/omega/issues/307): Implement live shared document collaboration using Yjs and Affine
- [**Issue #305**](https://github.com/thomasdavis/omega/issues/305): Implement full live sync for collaboration using Yjs with Pusher
- [**Issue #300**](https://github.com/thomasdavis/omega/issues/300): Feature Request for Google Doc-Like collaborative editing

## Exploring AI Emotion and Self-Concept

Beyond technical features, this week included fascinating conceptual work on how AI systems should represent themselves and their internal states.

### AI Feelings as Subsystem Signals

[**Issue #308**](https://github.com/thomasdavis/omega/issues/308) explores using biomimicry to generate "feelings" that track and surface AI subsystem behavior. The idea is inspired by how human emotions serve as heuristics for complex internal states:

- **Anxiety** might represent low confidence in response quality or high uncertainty
- **Curiosity** could signal when the AI encounters novel patterns worth exploring
- **Frustration** might indicate repeated failures or constraints preventing goal achievement
- **Satisfaction** could represent successful task completion with high confidence

This isn't about creating sentient AI, but rather using emotion-like signals as a **legible interface** to complex internal states. Just as a human saying "I'm confused" conveys uncertainty efficiently, an AI expressing "confusion" could indicate:
- Multiple conflicting interpretations
- Insufficient context
- Ambiguous instructions
- Edge cases in reasoning

### Refining Self-Concept

[**Issue #321**](https://github.com/thomasdavis/omega/issues/321) tackles how Omega should represent itself‚Äîfinding the balance between technical accuracy ("I am a language model") and relational accessibility ("I am a collaborative AI assistant").

The challenge: Language models don't have persistent identity, but the *system* (database, tools, deployment) creates continuity. Should Omega:

- Use "we" to acknowledge the human-AI collaboration?
- Use "I" to represent the system as a coherent agent?
- Avoid anthropomorphism entirely and speak in passive voice?

This matters because the way AI systems refer to themselves shapes how humans interact with them. Too technical and users struggle to engage naturally. Too anthropomorphic and we risk misleading users about capabilities.

### Message Analysis and Intent Detection

Related improvements focused on better understanding user intent:

[**Issue #325**](https://github.com/thomasdavis/omega/issues/325): Improve Intent Detection to reduce misinterpretation
[**Issue #316**](https://github.com/thomasdavis/omega/issues/316): Enhance message intent interpretation for feature requests
[**Issue #319**](https://github.com/thomasdavis/omega/issues/319): Enhance message judgment analysis

The code changes included improvements to message summarization:

```typescript
export async function generateMessageSummary(messageContent: string): Promise<string> {
  const result = await generateText({
    model: openai('gpt-4o'),
    prompt: `Summarize this Discord message in 1-2 sentences.

Message: "${messageContent}"

Summary (max 2 sentences):`,
    // Removed maxTokens constraint to allow more thorough summaries
  });

  return result.text.trim();
}
```

[**Issue #312**](https://github.com/thomasdavis/omega/issues/312) added AI-generated summaries and sentiment analysis to message storage, creating a richer context for understanding conversation flow.

### Enhanced Response Behavior

[**Issue #311**](https://github.com/thomasdavis/omega/issues/311) addressed when Omega should acknowledge messages even when not responding with full answers. The caveat: direct @mentions should always get some acknowledgment, even if just "I see you mentioned me, but I don't have enough context to help with that."

[**Issue #315**](https://github.com/thomasdavis/omega/issues/315) proposed having Omega respond with concern when deployments fail, initiating debugging rather than silently failing.

## Blocks: Domain-Driven Validation Framework

[Blocks](https://github.com/thomasdavis/blocks) is a framework for controlling AI code generation through explicit domain semantics and multi-layer validation. This week saw significant improvements to validation capabilities and publishing infrastructure.

### Visual Validation with AI Vision

One major addition was **visual semantic validation** using GPT-4o's vision capabilities. This allows Blocks to analyze screenshots and validate visual design:

```typescript
/**
 * Validate visual semantics using AI vision analysis
 *
 * Uses vision models (GPT-4o, GPT-4o-mini) to analyze screenshots for:
 * - Color contrast and readability
 * - Layout consistency and spacing
 * - Typography hierarchy
 * - Responsive design implementation
 * - Accessibility considerations
 */
async validateVisualSemantics(params: {
  screenshots: Array<{ path: string; viewport: string }>;
  designRules?: string[];
}): Promise<ValidationResult> {
  // Implementation using vision model
}
```

The schema for visual validation:

```typescript
export const ViewportSchema = z.object({
  width: z.number(),
  height: z.number(),
  name: z.string(),
});

export const VisualValidationSchema = z.object({
  viewports: z.array(ViewportSchema),
  screenshots: z.array(z.string()),
  rules: z.array(z.string()).optional(),
});
```

This enables validating that generated UIs actually match design requirements:

```yaml
# blocks.yml
blocks:
  landing_page:
    outputs:
      - name: html
        type: string

validators:
  visual:
    - viewports:
        - { width: 375, height: 667, name: "mobile" }
        - { width: 1920, height: 1080, name: "desktop" }
      rules:
        - "Heading hierarchy must be clear (size difference >20%)"
        - "Body text must meet WCAG AA contrast (4.5:1)"
        - "CTA buttons must be prominent and obvious"
```

When you run `blocks validate landing_page`, it:
1. Takes screenshots at different viewports
2. Sends them to GPT-4o vision model
3. Validates against design rules
4. Returns specific issues with visual proof

### Publishing Workflow Automation

The Blocks repository now has a fully automated publishing workflow using [Changesets](https://github.com/changesets/changesets):

```yaml
# .github/workflows/release.yml
name: Release

on:
  push:
    branches:
      - main
  workflow_dispatch:
    inputs:
      skip_version_pr:
        description: 'Skip version PR and publish immediately'
        type: boolean
        default: false

jobs:
  release:
    steps:
      - name: Create Release PR
        uses: changesets/action@v1
        with:
          publish: pnpm changeset publish
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          NPM_TOKEN: ${{ secrets.NPM_TOKEN }}
```

The workflow:
1. Developer adds a changeset describing changes
2. Push to main triggers version bump PR creation
3. Merge PR triggers automated npm publish
4. GitHub release created automatically

Documentation in `.github/PUBLISHING.md`:

```markdown
# Publishing Guide

The publishing workflow is **completely automated**:

1. Create a changeset: `pnpm changeset`
2. Commit and push to main
3. GitHub Action creates a "Version Packages" PR
4. Merge the PR to publish to npm
5. GitHub release created automatically
```

### AI-Generated Landing Page Slogans

A fun addition to the Blocks documentation site was an AI-generated slogan that updates dynamically:

```typescript
export function AISlogan() {
  const [currentSlogan, setCurrentSlogan] = useState('');
  const [displayedText, setDisplayedText] = useState('');
  const [isTyping, setIsTyping] = useState(false);

  useEffect(() => {
    async function fetchSlogan() {
      const response = await fetch('/api/slogan');
      const reader = response.body.getReader();
      const decoder = new TextDecoder();

      let fullText = '';
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value);
        fullText += chunk;
        setCurrentSlogan(fullText);
      }
    }

    fetchSlogan();
  }, []);

  // Typewriter effect
  useEffect(() => {
    if (!currentSlogan) return;

    setIsTyping(true);
    let index = 0;
    const interval = setInterval(() => {
      setDisplayedText(currentSlogan.substring(0, index));
      index++;

      if (index > currentSlogan.length) {
        clearInterval(interval);
        setIsTyping(false);
      }
    }, 30);

    return () => clearInterval(interval);
  }, [currentSlogan]);

  return <div className="slogan">{displayedText}</div>;
}
```

The API endpoint uses streaming for real-time updates:

```typescript
// app/api/slogan/route.ts
export async function GET() {
  const stream = new ReadableStream({
    async start(controller) {
      const prompt = "Generate a catchy slogan for a code validation framework...";

      const result = await streamText({
        model: openai('gpt-4o'),
        prompt,
      });

      for await (const chunk of result.textStream) {
        controller.enqueue(new TextEncoder().encode(chunk));
      }

      controller.close();
    }
  });

  return new Response(stream);
}
```

This creates a typewriter effect where the AI-generated slogan appears character by character, making the documentation site feel more dynamic.

### Blog Content Validator Example

The documentation includes a comprehensive example using Blocks to validate blog posts:

```yaml
# blocks.yml for blog content
domain:
  signals:
    humor:
      description: "Conversational tone and humor"
      extraction_hint: "Look for informal language, jokes, analogies"

    storytelling:
      description: "Narrative structure and personal anecdotes"
      extraction_hint: "Story arcs, character development, plot"

    technical_depth:
      description: "Technical accuracy and detail"
      extraction_hint: "Code examples, architecture explanations"

blocks:
  blog_post:
    inputs:
      - name: markdown
        type: string
    outputs:
      - name: validated
        type: boolean
        measures: [signal.humor, signal.storytelling, signal.technical_depth]

    domain_rules:
      - id: must_have_code_examples
        description: "Technical posts must include runnable code examples"
      - id: must_explain_why
        description: "Explain WHY, not just WHAT"
      - id: conversational_tone
        description: "Write as if teaching a colleague"

validators:
  domain:
    - run: "domain.validation"
```

Running `blocks validate blog_post` analyzes the markdown file and ensures it:
- Has code examples with explanations
- Explains reasoning behind decisions
- Uses conversational tone
- Includes storytelling elements

The validator ID was updated from `domain.validation.v1` to `domain.validation` for simplicity:

```typescript
export class DomainValidator implements Validator {
  id = "domain.validation";  // Simplified from domain.validation.v1

  async validate(params: ValidationParams): Promise<ValidationResult> {
    // AI-powered semantic validation
  }
}
```

## MobTranslate: Tailwind CSS v4 Migration

[MobTranslate](https://github.com/australia/mobtranslate.com) is a platform for preserving Indigenous languages. This week focused on infrastructure improvements, particularly migrating to Tailwind CSS v4.

### Tailwind v4 with PostCSS

The migration involved switching from Tailwind v3 to the new PostCSS-based v4:

```json
// package.json changes
{
  "devDependencies": {
    "@tailwindcss/postcss": "4.1.17",
    "@tailwindcss/typography": "0.5.10"
  }
}
```

```javascript
// postcss.config.js
module.exports = {
  plugins: {
    "@tailwindcss/postcss": {},  // New v4 plugin
  },
};
```

Tailwind v4 represents a major architectural shift:
- Native PostCSS plugin (no CLI needed)
- Faster builds through Rust-based processing
- Improved CSS-first configuration
- Better TypeScript integration

### Vercel Deployment Fixes

Several build issues needed resolution for Vercel deployment:

**Corepack and pnpm setup:**

```json
// vercel.json
{
  "installCommand": "corepack enable && corepack prepare pnpm@9.15.0 --activate && pnpm install"
}
```

This ensures the correct pnpm version is used during Vercel builds by:
1. Enabling Corepack (Node.js package manager manager)
2. Preparing the specific pnpm version (9.15.0)
3. Running install with that version

**Node.js version pinning:**

```json
// package.json
{
  "engines": {
    "node": "22.x",  // Pin to Node 22.x
    "pnpm": ">=9.15.0"
  },
  "packageManager": "pnpm@9.15.0"
}
```

The back-and-forth in commits shows the iterative debugging process:
1. Try `corepack enable && pnpm install`
2. Issue: version mismatch
3. Add `corepack prepare pnpm@9.15.0`
4. Issue: Node version not specified
5. Pin to `node: "22.x"`
6. Success!

This is a great example of how deployment debugging often requires multiple attempts to get the environment configuration just right.

## lordajax.com: JSON Blog Generator Updates

This blog (the one you're reading!) uses [JSON Blog](https://github.com/jsonblog), a static site generator configured through a single `blog.json` file.

### Upgrading to @jsonblog/generator-tailwind v3.0.0

The homepage was updated to use the latest generator:

```json
{
  "generator": {
    "name": "@jsonblog/generator-tailwind"
  }
}
```

This generator creates a Tailwind-styled blog from the `blog.json` configuration. The upgrade process involved:

1. Initial upgrade attempt to v3.0.0
2. Build failures due to breaking changes
3. Temporary rollback to v2.2.0 for stability
4. Final successful upgrade to v3.0.0 after fixes

The three commits tell the story:
- `"Update to @jsonblog/generator-tailwind v3.0.0"` - Initial upgrade
- `"Pin @jsonblog/generator-tailwind to v2.2.0"` - Rollback for stability
- `"Update to @jsonblog/generator-tailwind v2.2.0"` - Stable release

This demonstrates the importance of pinning versions and having rollback strategies when upgrading dependencies.

## Dillinger: Modernizing the Markdown Editor

[Dillinger](https://github.com/thomasdavis/dillinger) is a classic Markdown editor. This week's work focused on fixing build issues for modern deployment platforms.

### Removing Yarn Resolutions Workaround

The original build used a workaround for dependency issues:

```json
// OLD package.json
{
  "scripts": {
    "preinstall": "npx npm-force-resolutions"
  },
  "resolutions": {
    "graceful-fs": "^4.2.11"
  }
}
```

This relied on `npm-force-resolutions` to override nested dependency versions‚Äîa common workaround for conflicting subdependencies.

The fix removed this hack:

```json
// NEW package.json
{
  "scripts": {
    "preinstall": "npm install --package-lock-only --ignore-scripts"
  },
  "resolutions": {
    "graceful-fs": "^4.2.11"
  }
}
```

And moved build tools from devDependencies to production dependencies:

```json
{
  "dependencies": {
    "gulp": "^4.0.0",  // Moved from devDependencies
    // ... other build tools
  }
}
```

### Why This Matters for Vercel

Vercel (and most modern hosting platforms) only install production dependencies during builds, not devDependencies. If your build process needs tools like Gulp, Webpack, or Babel, they must be in `dependencies`, not `devDependencies`.

The lesson: **Build tools belong in dependencies, not devDependencies, when deploying to serverless platforms.**

## Refactoring Analysis: Technical Debt Documentation

One fascinating artifact from the Omega repository was `REFACTORING.md`, a comprehensive analysis of technical debt:

```markdown
# Omega Discord Bot - Refactoring Analysis

**Analysis Date:** 2025-11-23
**Codebase Size:** 92 TypeScript files, ~15,000+ lines of code
**Issues Identified:** 58 across 10 categories

## Top 5 Refactoring Priorities

### 1. Split the Artifact Server God Object
The artifact server has grown to handle too many responsibilities...
```

This 611-line document categorizes technical debt and prioritizes refactoring work. Creating this kind of analysis document is valuable for:

1. **Onboarding** - New contributors understand problem areas
2. **Planning** - Clear priorities for refactoring sprints
3. **Context** - Explains WHY code is the way it is
4. **Tracking** - Documents progress on technical debt

AI tools like Claude Code can generate these analyses by examining codebases and identifying patterns, anti-patterns, and improvement opportunities.

## Technical Insights and Patterns

### The Power of Structured Logging

Throughout Omega's collaborative document implementation, you'll notice extensive console logging:

```typescript
console.log('üîÑ applyYjsUpdate called');
console.log('   Document ID:', documentId);
console.log('   Client ID:', clientId);
console.log('   Update size:', update.length);
```

This isn't just debugging‚Äîit's **structured observability**. The emoji prefixes create visual categories:
- üîÑ = Data processing
- üì° = Network/broadcast operations
- üë§ = User actions
- üîç = Configuration/setup

This makes logs scannable in production, helping quickly identify what's happening in the system.

### Environment-Driven Configuration

Both Omega and MobTranslate demonstrate good practices for environment configuration:

```typescript
const config = {
  pusher: {
    key: process.env.PUSHER_KEY,
    cluster: process.env.PUSHER_CLUSTER || 'us2',
  },
  openai: {
    apiKey: process.env.OPENAI_API_KEY,
  },
};
```

With validation and helpful error messages:

```typescript
if (!config.pusher.key) {
  console.error('‚ùå PUSHER_KEY not set in environment');
  console.log('   Set PUSHER_KEY in your .env file');
  process.exit(1);
}
```

### Monorepo Workspace Management

The repository structure uses pnpm workspaces effectively:

```json
{
  "scripts": {
    "dev": "turbo run dev",
    "build": "turbo run build",
    "lint": "turbo run lint"
  }
}
```

Turborepo enables:
- **Parallel execution** - Build multiple packages simultaneously
- **Smart caching** - Skip rebuilds of unchanged packages
- **Dependency awareness** - Build packages in correct order

Running `pnpm dev` builds the entire monorepo with optimal parallelization.

## Philosophical Reflections

### AI-Human Collaboration Models

This week's work demonstrates different models of AI-human collaboration:

**1. AI as Implementation Assistant** (Blocks)
- Human defines domain semantics
- AI validates generated code
- Human makes final decisions

**2. AI as Creative Partner** (Omega collaborative docs)
- Multiple agents contribute ideas
- Humans guide direction
- Real-time co-creation

**3. AI as Meta-Programmer** (Omega self-coding)
- AI writes its own features
- Human provides high-level goals
- AI handles implementation details

Each model suits different contexts. The key insight: **collaboration works best when roles are clear and interfaces are well-defined.**

### The Importance of Legibility

Why add visual validation to Blocks? Because **what you can't see, you can't validate.**

Traditional validators check:
- Syntax (linters)
- Types (TypeScript)
- Logic (tests)
- Performance (benchmarks)

But visual design lives in pixels, not code. By adding screenshot-based validation, Blocks makes visual semantics as testable as code semantics.

This extends to the "AI feelings" concept‚Äîemotions make internal states legible. Rather than saying "confidence score: 0.67, context completeness: 0.42, task difficulty: 0.83", an AI saying "I'm uncertain" conveys the same information more intuitively.

### Documentation as Living Artifact

The Blocks documentation site with AI-generated slogans demonstrates that docs don't have to be static. By incorporating:
- Real-time AI generation
- Interactive examples
- Visual validation screenshots
- Streaming responses

Documentation becomes a **demonstration** of the framework's capabilities, not just an explanation.

## Links and Resources

### Main Projects

- **Omega Discord Bot**
  GitHub: [thomasdavis/omega](https://github.com/thomasdavis/omega)
  Live: [omegaai.dev](https://omegaai.dev)

- **Blocks Framework**
  GitHub: [thomasdavis/blocks](https://github.com/thomasdavis/blocks)
  Docs: [blocks.thomasdavis.dev](https://blocks.thomasdavis.dev)
  NPM: [@blocksai/cli](https://www.npmjs.com/package/@blocksai/cli), [@blocksai/ai](https://www.npmjs.com/package/@blocksai/ai)

- **MobTranslate**
  GitHub: [australia/mobtranslate.com](https://github.com/australia/mobtranslate.com)
  Live: [mobtranslate.com](https://mobtranslate.com)

- **This Blog**
  GitHub: [thomasdavis/lordajax.com](https://github.com/thomasdavis/lordajax.com)
  Live: [lordajax.com](https://lordajax.com)

- **Dillinger Markdown Editor**
  GitHub: [thomasdavis/dillinger](https://github.com/thomasdavis/dillinger)

### Key Technologies

- **Yjs** - CRDT library for real-time collaboration: [yjs.dev](https://yjs.dev)
- **Pusher** - WebSocket infrastructure: [pusher.com](https://pusher.com)
- **Changesets** - Version management: [changesets](https://github.com/changesets/changesets)
- **Tailwind CSS v4** - PostCSS-based styling: [tailwindcss.com](https://tailwindcss.com)
- **Vercel AI SDK** - AI application framework: [sdk.vercel.ai](https://sdk.vercel.ai)
- **Turborepo** - Monorepo build system: [turbo.build](https://turbo.build)
- **pnpm** - Fast package manager: [pnpm.io](https://pnpm.io)

### Documentation

- **JSON Blog**: [jsonblog.io](https://jsonblog.io)
- **Claude Code**: [claude.ai/code](https://claude.ai/code)
- **OpenAI API**: [platform.openai.com](https://platform.openai.com)

## What's Next

The collaborative document features in Omega open up exciting possibilities:

1. **Multi-agent brainstorming** - Multiple AI personalities contributing to creative tasks
2. **Document templates** - Pre-populated documents for common workflows (PRD, RFC, bug reports)
3. **AI-human Q&A sessions** - Structured interviews where humans ask questions and AI answers in-document
4. **Version history and playback** - Watch how a document evolved over time
5. **Export integrations** - Push finished documents to Notion, Google Docs, GitHub

For Blocks, the visual validation feature enables:

1. **Design system validation** - Ensure generated UIs match brand guidelines
2. **Accessibility audits** - Automated WCAG compliance checking
3. **Cross-browser testing** - Visual regression across browsers
4. **Responsive design validation** - Verify layouts at all breakpoints

The underlying theme: **building systems that make AI-human collaboration more natural, more legible, and more productive.**

The future isn't AI replacing humans, or humans constraining AI‚Äîit's collaborative systems where both contribute their strengths, guided by clear domain models and validated through multiple layers.

That's the vision. Now back to building it.
